<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yudong Guo</title>

  <meta name="author" content="Yudong Guo">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yudong Guo</name>
                  </p>
                  <p>I am a Ph.D. student in school of Mathematical Sciences at <a
                      href="http://en.ustc.edu.cn/">USTC</a>, supervised by <a
                      href="http://staff.ustc.edu.cn/~juyong/">Prof. Juyong Zhang</a>. Before that, I received bachelor
                    degree in Statistics in 2015 from University of Science and Technology of China (USTC). From fall
                    2016 to spring 2017, I was a research assistant in the MultiMedia Lab at Nanyang Technological
                    University under supervision of <a href="https://jianfei-cai.github.io/">Prof. Jianfei Cai</a> and
                    <a href="https://personal.ntu.edu.sg/asjmzheng/">Prof. Jianmin Zheng</a>. My
                    research interests including 3D vision and digital human.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:gyd2011@mail.ustc.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=cxF_-i4AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/YudongGuo/">Github</a>
                  </p>
                </td>
                <!-- <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/JonBarron_circle.jpg" class="hoverZoomLink"></a>
                </td> -->
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="vertical-align:middle">
                  <img src='images/AD-NeRF.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2103.11078">
                    <papertitle>AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</papertitle>
                  </a>
                  <br>
                  <strong>Yudong Guo</strong>, Keyu Chen, Sen Liang, <a
                    href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/Yongjin.htm">Yongjin Liu</a>, <a
                    href="http://www.cad.zju.edu.cn/bao/">Hujun Bao</a>, <a
                    href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>
                  <br>
                  <em>arXiv </em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2103.11078">paper</a> /
                  <a href="https://www.youtube.com/watch?v=TQO2EBYXLyU">video</a>
                  <p></p>
                  <p>In this paper, we address the talking head problem with the aid of neural scene representation
                    networks. Our method is completely different from existing methods that rely on intermediate
                    representations like 2D landmarks or 3D face models to bridge the gap between audio input and video
                    output. Specifically, the feature of input audio signal is directly fed into a conditional implicit
                    function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video
                    corresponding to the audio signal is synthesized using volume rendering.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/FaceFromX.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1808.05323">
                    <papertitle>3D Face From X: Learning Face Shape from Diverse Sources</papertitle>
                  </a>
                  <br>
                  <strong>Yudong Guo</strong>, Lin Cai, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>
                  <br>
                  <em>IEEE Transactions on Image Processing (TIP) </em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/1808.05323">paper</a> /
                  <a href="data/FaceFromX.mp4">video</a>
                  <p></p>
                  <p>We present a novel method to jointly learn a 3D face parametric model and 3D face reconstruction
                    from diverse sources. Besides scanned face data and face images, we also utilize a large number of
                    RGB-D images captured with an iPhone X to bridge the gap between the two sources.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/FaceCorrection.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Real-Time Face View Correction for Front-Facing Cameras</papertitle>
                  <br>
                  <strong>Yudong Guo</strong>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, Yihua Chen,
                  Hongrui Cai, <a href="http://staff.ustc.edu.cn/~zhuang/">Zhangjin Huang</a>, <a
                    href="http://www.bdeng.me/">Bailin Deng</a>
                  <br>
                  <em>Computational Visual Media </em>, 2021
                  <br>
                  <a href="data/FaceViewCorrection.pdf">paper</a> /
                  <a href="data/FaceViewCorrection.mp4">video</a>
                  <p></p>
                  <p>We takes the video stream from a single RGB camera as input, and generates a video stream that
                    emulates the view from a virtual camera at a designated location. Our system works well for
                    different lighting conditions and skin tones, and is able to handle users wearing glasses.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/StereoPiFU.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision</papertitle>
                  <br>
                  <a href="http://hy1995.top/">Yang Hong</a>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong
                    Zhang</a>, Boyi Jiang, <strong>Yudong
                    Guo</strong>, <a href="http://staff.ustc.edu.cn/~lgliu/">Ligang Liu</a>, <a
                    href="http://www.cad.zju.edu.cn/bao/">Hujun Bao</a>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2021
                  <br>
                  <a href="">paper</a> (comming soon) /
                  <a href="">code</a> (comming soon) /
                  <a href="https://1drv.ms/v/s!AvRQvmQWxdtwkQhsCmcLK-bg3IOZ?e=xxXbCe">video</a>
                  <p></p>
                  <p>We propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit
                    function representation of PIFu, to recover the 3D shape of the clothed human. StereoPIFu can
                    naturally infer the human body's spatial location in camera space and maintain the correct relative
                    position of different parts of the human body, which enables our method to capture human
                    performance.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/Caricature.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2004.09190">
                    <papertitle>Landmark Detection and 3D Face Reconstruction for Caricature using a Nonlinear
                      Parametric
                      Model</papertitle>
                  </a>
                  <br>
                  Hongrui Cai, <strong>Yudong Guo</strong>, Zhuang Peng, <a
                    href="http://staff.ustc.edu.cn/~juyong/">Juyong
                    Zhang</a>
                  <br>
                  <em>Graphical Models </em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2004.09190">paper</a> /
                  <a href="https://github.com/Juyong/CaricatureFace">code</a>
                  <p></p>
                  <p>We propose the first automatic method for landmark detection and 3D face reconstruction for
                    caricature by a
                    novel 3D approach. To this end, we first
                    build a dataset with various styles of 2D caricatures and their corresponding 3D shapes, and then
                    build a parametric model on vertex based deformation space for 3D caricature face.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/FacePS.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2003.12307">
                    <papertitle>Lightweight Photometric Stereo for Facial Details Recovery</papertitle>
                  </a>
                  <br>
                  Xueying Wang, <strong>Yudong Guo</strong>, <a href="http://www.bdeng.me/">Bailin Deng</a>, <a
                    href="http://staff.ustc.edu.cn/~juyong/">Juyong
                    Zhang</a>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2003.12307">paper</a> /
                  <a href="https://github.com/Juyong/FacePSNet">code</a>
                  <p></p>
                  <p>We present a lightweight strategy that only requires sparse inputs or even a single image to
                    recover high-fidelity face shapes with images captured under near-field lights. To this end, we
                    construct a dataset containing 84 different subjects with 29 expressions under 3 different lights.
                    Data augmentation is applied to enrich the data in terms of diversity in identity, lighting,
                    expression, etc.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/CNNFace.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1708.00980">
                    <papertitle>CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face
                      Images</papertitle>
                  </a>
                  <br>
                  <strong>Yudong Guo</strong>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, <a
                    href="https://jianfei-cai.github.io/">Jianfei Cai</a>, Boyi Jiang, <a
                    href="https://personal.ntu.edu.sg/asjmzheng/">Jianmin Zheng</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </em>, 2018
                  <br>
                  <a href="https://arxiv.org/abs/1708.00980">paper</a> /
                  <a href="data/3DFace.mp4">video</a> /
                  <a href="https://github.com/Juyong/3DFace">dataset</a>
                  <p></p>
                  <p>With the powerfulness of convolution neural networks (CNN), CNN based face reconstruction has
                    recently shown promising performance in reconstructing detailed face shape from 2D face images. The
                    success of CNN-based methods relies on a large number of labeled data. The state-of-the-art
                    synthesizes such data using a coarse morphable face model, which however has difficulty to generate
                    detailed photo-realistic images of faces (with wrinkles). This paper presents a novel face data
                    generation method.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center">
                    <br>
                    Wonderful template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>