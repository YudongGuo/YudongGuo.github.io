<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yudong Guo</title>

  <meta name="author" content="Yudong Guo">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yudong Guo</name>
                  </p>
                  <p>I am a Ph.D. student in school of Mathematical Sciences at <a
                      href="http://en.ustc.edu.cn/">USTC</a>, supervised by <a
                      href="http://staff.ustc.edu.cn/~juyong/">Prof. Juyong Zhang</a>. Before that, I received bachelor
                    degree in Statistics in 2015 from University of Science and Technology of China (USTC). From fall
                    2016 to spring 2017, I was a research assistant in the MultiMedia Lab at Nanyang Technological
                    University under supervision of <a href="https://jianfei-cai.github.io/">Prof. Jianfei Cai</a> and
                    <a href="https://personal.ntu.edu.sg/asjmzheng/">Prof. Jianmin Zheng</a>. My
                    research interests include 3D vision and digital human.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:gyd2011@mail.ustc.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=cxF_-i4AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/YudongGuo/">Github</a>
                  </p>
                </td>
                <!-- <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/JonBarron_circle.jpg" class="hoverZoomLink"></a>
                </td> -->
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="vertical-align:middle">
                  <img src='images/AudioNeRF.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2103.11078">
                    <papertitle>AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</papertitle>
                  </a>
                  <br>
                  <strong>Yudong Guo</strong>, Keyu Chen, Sen Liang, <a
                    href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/Yongjin.htm">Yongjin Liu</a>, <a
                    href="http://www.cad.zju.edu.cn/bao/">Hujun Bao</a>, <a
                    href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>
                  <br>
                  <em>arXiv </em>, 2021
                  <br>
                  <a href="https://yudongguo.github.io/ADNeRF/">project page</a> /
                  <a href="https://arxiv.org/abs/2103.11078">paper</a> /
                  <a href="https://github.com/YudongGuo/AD-NeRF">code</a> /
                  <a href="https://www.youtube.com/watch?v=TQO2EBYXLyU">video</a>
                  <p></p>
                  <p>We address the talking head problem with the aid of neural scene representation
                    networks. The feature of audio is fed into a conditional implicit function to generate a dynamic
                    neural radiance field for high-fidelity talking-head video synthesis.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/FaceFromX.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1808.05323">
                    <papertitle>3D Face From X: Learning Face Shape from Diverse Sources</papertitle>
                  </a>
                  <br>
                  <strong>Yudong Guo</strong>, Lin Cai, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>
                  <br>
                  <em>IEEE Transactions on Image Processing (TIP) </em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/1808.05323">paper</a> /
                  <a href="data/FaceFromX.mp4">video</a>
                  <p></p>
                  <p>We present a method to jointly learn a 3D face parametric model and 3D face reconstruction
                    from diverse sources including scanned 3D faces, in-the-wild facial images and RGB-D images captured
                    with an iPhone X.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/FaceCorrection.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Real-Time Face View Correction for Front-Facing Cameras</papertitle>
                  <br>
                  <strong>Yudong Guo</strong>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, Yihua Chen,
                  <a href="https://github.com/RainbowRui">Hongrui Cai</a>, <a href="http://staff.ustc.edu.cn/~zhuang/">Zhangjin Huang</a>, <a
                    href="http://www.bdeng.me/">Bailin Deng</a>
                  <br>
                  <em>Computational Visual Media </em>, 2021
                  <br>
                  <a href="data/FaceViewCorrection.pdf">paper</a> /
                  <a href="data/FaceViewCorrection.mp4">video</a>
                  <p></p>
                  <p>We takes the video stream from a single RGB camera as input, and generates a video stream that
                    emulates the view from a virtual camera at a designated location.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/StereoPiFU.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision</papertitle>
                  <br>
                  <a href="http://hy1995.top/">Yang Hong</a>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong
                    Zhang</a>, Boyi Jiang, <strong>Yudong
                    Guo</strong>, <a href="http://staff.ustc.edu.cn/~lgliu/">Ligang Liu</a>, <a
                    href="http://www.cad.zju.edu.cn/bao/">Hujun Bao</a>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2021
                  <br>
                  <a href="http://hy1995.top/StereoPIFuProject/">project page</a> /
                  <a href="https://arxiv.org/abs/2104.05289">paper</a> /
                  <a href="https://github.com/CrisHY1995/StereoPIFu_Code">code</a> 
                  <p></p>
                  <p>We propose StereoPIFu, which integrates the geometric constraints of stereo vision with implicit
                    function representation of PIFu, to recover the 3D shape of the clothed human.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/Caricature.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2004.09190">
                    <papertitle>Landmark Detection and 3D Face Reconstruction for Caricature using a Nonlinear
                      Parametric
                      Model</papertitle>
                  </a>
                  <br>
                  <a href="https://github.com/RainbowRui">Hongrui Cai</a>, <strong>Yudong Guo</strong>, Zhuang Peng, <a
                    href="http://staff.ustc.edu.cn/~juyong/">Juyong
                    Zhang</a>
                  <br>
                  <em>Graphical Models </em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2004.09190">paper</a> /
                  <a href="https://github.com/Juyong/CaricatureFace">code</a>
                  <p></p>
                  <p>We propose the first automatic method for landmark detection and 3D face reconstruction for
                    caricature by a 3D approach.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/FacePS.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2003.12307">
                    <papertitle>Lightweight Photometric Stereo for Facial Details Recovery</papertitle>
                  </a>
                  <br>
                  Xueying Wang, <strong>Yudong Guo</strong>, <a href="http://www.bdeng.me/">Bailin Deng</a>, <a
                    href="http://staff.ustc.edu.cn/~juyong/">Juyong
                    Zhang</a>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2003.12307">paper</a> /
                  <a href="https://github.com/Juyong/FacePSNet">code</a>
                  <p></p>
                  <p>We present a lightweight strategy that only requires sparse inputs or even a single image to
                    recover high-fidelity face shapes with images captured under near-field lights.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:0px;width:25%;vertical-align:middle">
                  <img src='images/CNNFace.png' style="width:200px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1708.00980">
                    <papertitle>CNN-based Real-time Dense Face Reconstruction with Inverse-rendered Photo-realistic Face
                      Images</papertitle>
                  </a>
                  <br>
                  <strong>Yudong Guo</strong>, <a href="http://staff.ustc.edu.cn/~juyong/">Juyong Zhang</a>, <a
                    href="https://jianfei-cai.github.io/">Jianfei Cai</a>, Boyi Jiang, <a
                    href="https://personal.ntu.edu.sg/asjmzheng/">Jianmin Zheng</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </em>, 2018
                  <br>
                  <a href="https://arxiv.org/abs/1708.00980">paper</a> /
                  <a href="data/3DFace.mp4">video</a> /
                  <a href="https://github.com/Juyong/3DFace">dataset</a>
                  <p></p>
                  <p>This paper presents a novel face data generation method to train the 3DFaceNet for real-time
                    dense face reconstruction from a monocular RGB video.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center">
                    <br>
                    Wonderful template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>